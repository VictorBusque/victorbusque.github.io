---
layout: post
title: "AI Joker: Las m√°quinas bromean"
---
Tags: [NLP](https://es.wikipedia.org/wiki/Procesamiento_de_lenguajes_naturales), [Language Model](https://es.wikipedia.org/wiki/Modelaci%C3%B3n_del_lenguaje), [Deep learning](https://es.wikipedia.org/wiki/Aprendizaje_profundo)
### Las m√°quinas no hacen gracia
Nos ha pasado a todos. Ya sea a Alexa, a Siri o al asistente de Google, todos alguna vez le hemos pedido a alguno de estos asistentes virtuales que nos cuenten un chiste. El resultado suele ser un chiste aleatorio preparado y por lo general bastante suave.
{: style="text-align: justify" }
En este contexto, surge la siguiente hip√≥tesis: ¬øPodr√≠a ense√±arle a una m√°quina a hacer chistes de temas concretos?
{: style="text-align: justify" }
En este post pretendo dar una visi√≥n t√©cnica sobre c√≥mo atacar esta hip√≥tesis.
{: style="text-align: justify" }

Disclaimer: Usar√© de forma indistinta "palabra" y "token" porque en el contexto de este proyecto son equivalentes, pero esto no sucede en todos los casos.
### Los datos
Como de costumbre, lo primero y m√°s importante son los datos. Si has le√≠do el post acerca de [modelos de lenguaje]({{site.baseurl}}/GPT3-language-models), entonces algo que est√° claro es que vamos a necesitar muchos datos si queremos entrenar un modelo de lenguaje.
{: style="text-align: justify" }
En este caso, despu√©s de buscar recursos online que pudieran ser √∫tiles, d√≠ con [este](https://github.com/taivop/joke-dataset) repositorio, que nos proporciona 3 ficheros con muchos chistes en ingl√©s extra√≠dos de internet, y adem√°s tambi√©n los distintos scrapers que los han generado en forma de scripts de Python. De los 3 ficheros con chistes ya extra√≠dos hemos usado s√≥lamente los siguientes dos:
* Banco de chistes de [Reddit](https://www.reddit.com/): 195.000 chistes, ~7.400.000 palabras.
* Banco de chistes de [Wocka](http://www.wocka.com/): 10.000 chistes, ~1.100.000 palabras.
{: style="text-align: justify" }
Los dataset tienen formato JSON, con objetos como el siguiente:

```json
{
  "title": "If I get a bird I'm naming it Trump",
  "body": "cuz all they do is Tweet",
  "id": "5tyyo2",
  "score": 1
}
```

### Preprocesado
Siendo un proyecto personal sin demasiada transcendencia, no me compliqu√©  demasiado la existencia en el momento de preprocesar/limpiar el texto y prepararlo para ser usado como entrada para entrenar el modelo.
{: style="text-align: justify" }
Principalmente el preprocesado que hice al texto de los chistes es, quitar todos los signos de puntuaci√≥n (excepto el ap√≥strofe, que en ingl√©s es fundamental), y convertirlo todo en min√∫sculas.
{: style="text-align: justify" }
### Vocabulario
Hay una decisi√≥n importante que tomar: ¬øQu√© presupuesto de vocabulario nos vamos a poner?
{: style="text-align: justify" }
Un presupuesto bajo en vocabulario (p.e. 1000) implicar√° que el modelo s√≥lo entender√° las 1000 palabras m√°s frecuentes entre las ~8.500.000 totales (99.914 distintas) que tenemos en el dataset, pero el resto no. Eso podr√≠a suponer generar chistes sin sentido por falta de palabras concretas que no sean excesivamente frecuentes. 
{: style="text-align: justify" }
En cambio, un presupuesto muy alto o ilimitado nos incrementar√° la cantidad de par√°metros del modelo much√≠simo sin necesidad, puesto que tal vez estemos teniendo en cuenta palabras que aparecen 1, 2 o 3 veces entre todos los chistes.
{: style="text-align: justify" }
En este sentido, por una lado nuestra matriz de embeddings tiene tama√±o VxE (d√≥nde V=tama√±o de vocabulario, E=tama√±o del vector/embedding), por lo que cada palabra distinta podr√≠a a√±adir E par√°metros m√°s al modelo. 
{: style="text-align: justify" }
Adem√°s, como que un modelo de lenguaje quiere predecir la siguiente palabra, se entrena como clasificador con tantas clases como palabras distintas conoce, por lo que el tama√±o del vocabulario tendr√° importante influencia en la cantidad de par√°metros de la capa de clasificaci√≥n final.
{: style="text-align: justify" }
En este caso, el presupuesto lo defin√≠ en 10.000 de forma arbitraria. Idealmente se deber√≠a analizar m√°s profundamente las m√©tricas de frecuencia de palabras en chistes, y las capacidades de computaci√≥n disponibles para establecer un presupuesto √≥ptimo.
{: style="text-align: justify" }
En este punto, las palabras que se escapan al presupuesto se sustituir√°n por un token gen√©rico "\<unk\>", que terminar√° dando lugar a un embedding bastante inespec√≠fico. Adem√°s, a√±adiremos los tokens de control "\<pad\>" y "\<endtoken\>" para homogeneizar el tama√±o de la entrada y controlar cuando se terminan los chistes respectivamente.
### Tama√±o de los chistes
Como restricci√≥n obligada por la tecnolog√≠a actual, debemos establecer un l√≠mite de palabras a los chistes. Despu√©s de hacer un an√°lisis en el conjunto de datos, obtuve el siguiente histograma (x: cantidad de palabras, y: cu√°ntos chistes tienen esa cantidad de palabras).
<div style="display: flex; justify-content: center;">
  <img src="{{site.baseurl}}/images/2020-12-03-AI-Joker-m√°quinas-bromean/sentence-length.png" />
</div>
<center>Histograma de tama√±o de chiste.</center>
Viendo lo siguiente, v√≠ que el promedio de longitud de chistes eran de 45 palabras, y que aproximadamente el 80% de los chistes no alcanzaban las 50 palabras, por lo que el l√≠mite lo puse all√≠. Como he comentado, los chistes m√°s cortos van a recibir padding, y los m√°s largos terminar√°n siendo cortados.
{: style="text-align: justify" }
### Caracterizaci√≥n (Featurization)
Como muchos modelos de _Deep Learning_ sobre texto, la caracterizaci√≥n en este caso se va a basar en el uso de _word embeddings_, es decir, vectores num√©ricos que representan a cada palabra, e intuitivamente aportan informaci√≥n sem√°ntica.
{: style="text-align: justify" }
Estos _word embeddings_ pueden, o ser entrenados juntamente con el modelo (lo que suele dar embeddings orientados a la tarea que se pretende resolver), o bien se puede partir de embeddings que la comunidad haya entrenado previamente. En este caso, debido a que el lenguaje usado en chistes es un lenguaje de dominio general, opt√© por esta segunda opci√≥n, y he partido de los embeddings preentrenados mediante [GloVe](https://github.com/stanfordnlp/GloVe) (de Stanford).
{: style="text-align: justify" }
Una vez sabemos c√≥mo vamos a representar cada palabra, hace falta generar los datos finales, las dos matrices que usaremos como datos de entrada. El secreto en este caso es que la predicci√≥n es la misma entrada, pero desplazada en la dimension temporal un paso en el futuro. Es decir, como salida a la tercera palabra de un chiste, querremos que nos de la cuarta palabra del mismo chiste, y as√≠ para todas.
### Modelo
Puesto que este proyecto tiene un tiempo ya, la tecnolog√≠a que us√© fue LSTM (_Long Short-Term Memory_), es decir, una red neuronal recurrente con mejoras para recordar mejor informaci√≥n pasada. Actualmente esta tecnolog√≠a va siendo reemplazada por el uso de _Transformers_, no obstante, puesto que los chistes tienden a ser textos cortos, las debilidades de los LSTM respecto a los _Transformer_ no deber√≠an ser muy evidentes. Tal vez pr√≥ximamente haga un art√≠culo de RNN vs Transformers para modelos de Deep Learning sobre texto.
{: style="text-align: justify" }
A partir de aqu√≠, el modelo es bastante directo. Es un modelo secuencial, que recibe como entrada un listado de enteros (cada entero identifica una palabra), que luego ser√°n sustitu√≠dos por el vector o embedding correspondiente, por lo que terminaremos con una matriz, en la que cada fila es el embedding de cada palabra que conforme la frase, y cada columna ser√° una dimensi√≥n del embedding.
{: style="text-align: justify" }
Este embedding ser√° el que llegar√° al LSTM. Debido a que es un LSTM unidireccional, en el momento de procesar la palabra _t_ del chiste, s√≥lo tendremos el contexto de las palabras _0..t-1_. Esto difiere del funcionamiento en otras tareas como NER, d√≥nde el BiLSTM es m√°s √∫til y nos permite explotar dependencias en el pasado y futuro.
{: style="text-align: justify" }
Aunque en tiempo de entrenamiento tengamos acceso al futuro, en tiempo de predicci√≥n eso no ser√° posible, por lo que nos vamos a conformar con tener contexto unidireccional. En este caso defin√≠ dos capas de LSTM unidireccionales, con 256 celdas LSTM en cada capa. De nuevo, hay mucho margen de maniobra con la cantidad de capas y de celdas por capa, pero de entrada esta complejidad me pareci√≥ adecuada.
{: style="text-align: justify" }
En estas celdas LSTM apliqu√© un ratio de _dropout_ del 25% con el inter√©s de controlar posibles sobreajustes durante el entrenamiento y que siempre repitiera chistes vistos.
{: style="text-align: justify" }
Finalmente, llegamos a la √∫ltima capa, que en este caso es una simple capa totalmente conectada (_fully-connected_ o _dense_) con tantas neuronas como tama√±o del vocabulario (es decir, 10.003), con la funci√≥n de activaci√≥n _softmax_ que nos permite que la salida sea una distribuci√≥n de probabilidad para cada posible palabra en cada punto del chiste.
### Entrenamiento
Para el entrenamiento se hicieron 4 rondas de 15 iteraciones, dedicando un 85% para entrenamiento y el 15% para validaci√≥n. En este caso se entren√≥ en la plataforma de Google [Colab](https://colab.research.google.com/). Como pod√©is ver en los logs, cada ronda tard√≥ ~1:20h. En este caso podemos ignorar las m√©tricas de _accuracy_, puesto que en el escenario de _language modelling_ no es necesariamente una m√©trica relevante. Ser√≠a interesante analizar la _perplexity_.

```
Train on 173031 samples, validate on 30536 samples
Epoch 1/15
173031/173031 [==============================] - 384s 2ms/sample - loss: 1.9144 - accuracy: 0.3048 - val_loss: 2.6002 - val_accuracy: 0.2864
Epoch 2/15
173031/173031 [==============================] - 326s 2ms/sample - loss: 1.9052 - accuracy: 0.3062 - val_loss: 2.6049 - val_accuracy: 0.2849
Epoch 3/15
173031/173031 [==============================] - 325s 2ms/sample - loss: 1.8963 - accuracy: 0.3077 - val_loss: 2.5892 - val_accuracy: 0.2882
Epoch 4/15
173031/173031 [==============================] - 325s 2ms/sample - loss: 1.8883 - accuracy: 0.3090 - val_loss: 2.5858 - val_accuracy: 0.2887
Epoch 5/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8810 - accuracy: 0.3103 - val_loss: 2.5799 - val_accuracy: 0.2899
Epoch 6/15
173031/173031 [==============================] - 328s 2ms/sample - loss: 1.8738 - accuracy: 0.3117 - val_loss: 2.5779 - val_accuracy: 0.2903
Epoch 7/15
173031/173031 [==============================] - 325s 2ms/sample - loss: 1.8673 - accuracy: 0.3127 - val_loss: 2.5698 - val_accuracy: 0.2915
Epoch 8/15
173031/173031 [==============================] - 325s 2ms/sample - loss: 1.8611 - accuracy: 0.3137 - val_loss: 2.5708 - val_accuracy: 0.2917
Epoch 9/15
173031/173031 [==============================] - 324s 2ms/sample - loss: 1.8552 - accuracy: 0.3147 - val_loss: 2.5665 - val_accuracy: 0.2921
Epoch 10/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8498 - accuracy: 0.3159 - val_loss: 2.5598 - val_accuracy: 0.2933
Epoch 11/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8445 - accuracy: 0.3165 - val_loss: 2.5625 - val_accuracy: 0.2934
Epoch 12/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8393 - accuracy: 0.3173 - val_loss: 2.5569 - val_accuracy: 0.2944
Epoch 13/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8349 - accuracy: 0.3183 - val_loss: 2.5549 - val_accuracy: 0.2945
Epoch 14/15
173031/173031 [==============================] - 323s 2ms/sample - loss: 1.8304 - accuracy: 0.3190 - val_loss: 2.5436 - val_accuracy: 0.2969
Epoch 15/15
173031/173031 [==============================] - 322s 2ms/sample - loss: 1.8259 - accuracy: 0.3198 - val_loss: 2.5468 - val_accuracy: 0.2962
```
### Resultados
Sin invertir demasiado tiempo en la parte de preprocesado, arquitecturas de DL, hiperpar√°metros ni entrenamiento, tenemos un modelo que, por un lado, es generalmente capaz de redactar frases en ingl√©s correctamente. Tambi√©n se nota que s√≥lamente ha visto chistes, porque le des la entrada que le des, tiende a terminar el chiste de una forma "chistesca", es decir, la forma de la frase resultante suele ser similar a muchos tipos de chistes conocidos.
{: style="text-align: justify" }
En este sentido, podemos ver en cada palabra que va a generar, qu√© opciones considera, c√≥mo valora cada una, y es alucinante ver que tiende a seleccionar palabras con todo el sentido del mundo, como podemos ver a continuaci√≥n.
<div style="display: flex; justify-content: center;">
  <img src="{{site.baseurl}}/images/2020-09-04-GPT3-language-models/AIJoker-screenshot.png" />
</div>
<center>Generaci√≥n de un chiste palabra a palabra.</center>
Aunque tambi√©n tenemos la posibilidad de pedirle que se deje de intermedios y nos de el chiste final. Por ejemplo, para la entradilla "what is the" su resultado es:
<center>
"what is the difference between a chickpea and a garbanzo bean i've never had a garbanzo bean on my face"
</center>
Si bien es raro porque tiene no conoce los signos de puntuaci√≥n, queda patente que sabe tanto escribir frases gramaticalmente correctas, y tiene esta tendencia de escribir en formas similares a muchos chistes.
{: style="text-align: justify" }
Es importante remarcar, que puesto que internet tiene tendencia a ser machista, sexista y racista, f√°cilmente los chistes resultantes lo son tambi√©n, por lo que no me hago responsable si luego los chistes ofenden a alguien... üôÑü§∑üèª‚Äç‚ôÇÔ∏è
{: style="text-align: justify" }
### Demo
Disclaimer 1: Esto est√° desplegado como un contenedor de Docker en una app Heroku gratuita, puede que la ejecuci√≥n sea lenta.
{: style="text-align: justify" }
Disclaimer 2: Como recordatorio, el modelo s√≥lo sabe crear chistes en ingl√©s, la frase de entrada deber√≠a estar en ingl√©s tambi√©n.
{: style="text-align: justify" }
<iframe id="ai-joker-demo"
    title="AI-Joker Demo"
    width="100%"
    height="520px"
    src="{{site.baseurl}}/demos/ai-joker/index.html">
</iframe>
### Plug
Si despu√©s de leer esto te ha parecido interesante, puedes echarle un vistazo al c√≥digo fuente del proyecto, que tengo publicado en github [aqu√≠](https://github.com/VictorBusque/AI-Joker). Consta de una parte de "Research" que est√° conformada por un Jupyter Notebook, y una parte de "Production" con un script mucho m√°s definido para hacer pruebas.
{: style="text-align: justify" }

